normal
{
  python rc/main.py \
   --trainset data/coqa.train.pipeline.json \
   --devset data/coqa.dev.pipeline.json \
   --n_history 0 --dir pipeline_models \
   --embed_file ~/data/glove.840B.300d.txt \
   --predict_raw_text n \
   --cuda_id 3

   python seq2seq/train.py \
   -data data/seq2seq-pipeline \
   -save_model pipeline_models/seq2seq_copy \
   -copy_attn -reuse_copy_attn \
   -word_vec_size 300 \
   -pre_word_vecs_enc data/seq2seq-pipeline.embed.enc.pt \
   -pre_word_vecs_dec data/seq2seq-pipeline.embed.dec.pt \
   -epochs 20 -gpuid 2 -seed 123 -log_file "log_pgnet.txt"
}

modify{
  normal
  7983
  "overall": {
    "em": 52.0,
    "f1": 61.1,
    "turns": 7983
  }

  notmodify
  879
  "overall": {
    "em": 35.4,
    "f1": 46.9,
    "turns": 879
  }
}

modify2
{

  trainデータに置いて、疑問詞を補ってから訓練した
  前のmodifyではスクリプトがバグっててうまく行かなかった
  これでは上手く行った。疑問詞が補ったものと、補ってないものの精度は同じぐらい
  seq2seqの翻訳時にdynamicdictが書いてなかったため補完したが、変わらなかった
    自動でdynamic_dict使ってるらしい

  python3 scripts/gen_pipeline_data.py \
   --data_file data/coqa-dev-modify.json \
   --output_file1 data/coqa.dev.pipeline.json \
   --output_file2 data/seq2seq-dev-pipeline

  python rc/main.py \
  --testset data/coqa.dev.pipeline.json \
  --n_history 0 \
  --pretrained pipeline_models --pretrained_model pipeline_models/params-interro-modify-0423.saved \
  --cuda_id 2

  python scripts/gen_pipeline_for_seq2seq.py \
  --data_file data/coqa.dev.pipeline.json \
  --output_file pipeline_models/pipeline-seq2seq-src.txt \
  --pred_file pipeline_models/predictions.json

  python seq2seq/translate.py \
  -model pipeline_models/seq2seq_copy_acc_85.04_ppl_2.19_e14.pt \
  -src pipeline_models/pipeline-seq2seq-src.txt \
  -output pipeline_models/pred.txt \
  -replace_unk -gpu 1

  python scripts/gen_seq2seq_output.py \
  --data_file data/coqa-dev-modify.json \
  --pred_file pipeline_models/pred.txt \
  --output_file pipeline_models/pipeline.prediction-modify.json

  python evaluate-v1.0.py \
  --data-file data/coqa-dev-modify.json \
  --pred-file pipeline_models/pipeline.prediction-modify.json

  beam_size 5

  all
  "overall": {
    "em": 52.4,
    "f1": 62.2,
    "turns": 7983
  }

  interro
  "overall": {
  "em": 46.1,
  "f1": 61.0,
  "turns": 879
  }

  restored interro
  "overall": {
    "em": 30.2,
    "f1": 41.3,
    "turns": 879
  }

  noninterro
  "overall": {
  "em": 53.2,
  "f1": 62.3,
  "turns": 7104
  }
}

normal
{
  疑問詞などはそのままで学習
  >>> Dev Epoch: [21 / 50]
  [2019-04-23 21:40:22,094 INFO] [predict-21] step: [0 / 249] | f1 = 33.51 | em = 27.34
  [2019-04-23 21:40:22,094 INFO] used_time: 0.18s
  [2019-04-23 21:41:01,998 INFO] Validation Epoch 21 -- F1: 47.13, EM: 37.23 --
  [2019-04-23 21:41:04,420 INFO] !!! Updated: F1: 47.13, EM: 37.23

  python3 scripts/gen_pipeline_data.py \
   --data_file data/coqa-dev-normal.json \
   --output_file1 data/coqa.dev.pipeline.json \
   --output_file2 data/seq2seq-dev-pipeline

  python rc/main.py \
  --testset data/coqa.dev.pipeline.json \
  --n_history 0 \
  --pretrained pipeline_models --pretrained_model pipeline_models/params-normal-0424.saved \
  --cuda_id 3

  python scripts/gen_pipeline_for_seq2seq.py \
  --data_file data/coqa.dev.pipeline.json \
  --output_file pipeline_models/pipeline-seq2seq-src.txt \
  --pred_file pipeline_models/predictions.json

  python seq2seq/translate.py \
  -model pipeline_models/seq2seq_copy_acc_84.75_ppl_2.26_e21.pt \
  -src pipeline_models/pipeline-seq2seq-src.txt \
  -output pipeline_models/pred.txt \
  -replace_unk -dynamic_dict -gpu 3

  python scripts/gen_seq2seq_output.py \
  --data_file data/coqa-dev-normal.json \
  --pred_file pipeline_models/pred.txt \
  --output_file pipeline_models/pipeline.prediction-normal.json

  python evaluate-v1.0.py \
  --data-file data/coqa-dev-normal.json \
  --pred-file pipeline_models/pipeline.prediction-normal.json

  all
  "overall": {
    "em": 52.3,
    "f1": 61.2,
    "turns": 7983
  }

  interro
  "overall": {
    "em": 36.2,
    "f1": 46.7,
    "turns": 879
  }

  restored-interro
  "overall": {
    "em": 38.1,
    "f1": 51.3,
    "turns": 879
  }

  noninterro
  "overall": {
    "em": 54.3,
    "f1": 63.0,
    "turns": 7104
  }

beam_sizeを2にして実験
{
  python translate.py \
  -src data/coqa-src-train.txt \
  -output pred.txt \
  -replace_unk -dynamic_dict \
  -model model_data/demo-model_step_28000_ppl_12.0609_acc_56.5888.pt --beamsize 2
  python prepro_modify.py
  prepro ...

  python scripts/gen_pipeline_data.py \
   --data_file data/coqa-dev-interro-beam2.json \
   --output_file1 data/coqa.dev.pipeline.json \
   --output_file2 data/seq2seq-dev-pipeline

  python rc/main.py \
  --testset data/coqa.dev.pipeline.json \
  --n_history 0 \
  --pretrained pipeline_models --pretrained_model pipeline_models/params-interro-beam2-0425.saved \
  --cuda_id $2

  python scripts/gen_pipeline_for_seq2seq.py \
  --data_file data/coqa.dev.pipeline.json \
  --output_file pipeline_models/pipeline-seq2seq-src.txt \
  --pred_file pipeline_models/predictions.json

  python seq2seq/translate.py \
  -model pipeline_models/seq2seq_copy_acc_84.71_ppl_2.18_e25.pt \
  -src pipeline_models/pipeline-seq2seq-src.txt \
  -output pipeline_models/pred.txt \
  -replace_unk -dynamic_dict -gpu $2

  python scripts/gen_seq2seq_output.py \
  --data_file data/coqa-dev-interro-beam2.json \
  --pred_file pipeline_models/pred.txt \
  --output_file pipeline_models/pipeline.prediction-interro-beam2.json

  [2019-04-25 19:23:20,652 INFO] [predict-15] step: [0 / 249] | f1 = 30.90 | em = 21.88
  [2019-04-25 19:23:20,652 INFO] used_time: 0.18s
  [2019-04-25 19:24:02,601 INFO] Validation Epoch 15 -- F1: 47.60, EM: 36.86 --
  [2019-04-25 19:24:05,148 INFO] !!! Updated: F1: 47.60, EM: 36.86

  beam_size 2
  all
  "overall": {
    "em": 52.3,
    "f1": 62.0,
    "turns": 7983
  }

  restored_interro
  "overall": {
    "em": 46.1,
    "f1": 61.0,
    "turns": 879
  }


  noninterro
  "overall": {
    "em": 53.1,
    "f1": 62.1,
    "turns": 7104
  }

  sh predict normal
  interro(normal)
  "overall": {
    "em": 32.2,
    "f1": 42.7,
    "turns": 879
  }
}

modify-interro
{
  theの連続を消すなどの修正後
  train:元のデータ、interroから復元した物を追加。もともと完全な文なども使用
  データ量は大体前の2倍
  sh prepro.sh modify-interro

  params-modify-interro-0506.saved
  seq2seq_copy_acc_84.90_ppl_2.40_e24.pt

  normal
  "overall": {
    "em": 52.1,
    "f1": 61.9,
    "turns": 7099
  }

  interro
  "overall": {
    "em": 31.4,
    "f1": 42.9,
    "turns": 884
  }

}

sentenceのみから質問文を生成
beamsize 2で実験
  bleuではbeamsize 2の方が精度が高かったが、不自然な文？
  質疑応答できるかを実験
