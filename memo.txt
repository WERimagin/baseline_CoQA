https://github.com/stanfordnlp/coqa-baselines

#corenlpで事前に疑問詞を分けたデータを作成する
python prepro_interro.py

#test:coqaから疑問詞のみの質問文と文や答えのデータを取り出し、seq2seq用に整形する
#train:coqaから疑問詞のみの以下略(学習の際に、生成した質問文を混ぜないと精度が落ちるから)
全てのデータを使って学習。データ数は多い方がいい。
python prepro_seq2seq.py

#Opennmtの訓練済みのseq2seqで質問文を予測する
python translate.py \
-src data/coqa-src-train.txt \
-output pred.txt \
-replace_unk -dynamic_dict \
-model model_data/

#生成した質問文をcoqa型のデータに直す
python prepro_modify.py

->coqa-train-modify.json
  coqa-dev-modify.json

これをDRQA+PGNETの方に渡す


prepro

pipeline

#trainデータのdrqa用の処理
python3 scripts/gen_pipeline_data.py \
--data_file data/coqa-train-modify.json \
--output_file1 data/coqa.train.pipeline.json \
--output_file2 data/seq2seq-train-pipeline

#testデータのdrqa用の処理
python3 scripts/gen_pipeline_data.py \
 --data_file data/coqa-dev-modify.json \
 --output_file1 data/coqa.dev.pipeline.json \
 --output_file2 data/seq2seq-dev-pipeline

#両方のデータのpgnet用の処理
python3 seq2seq/preprocess.py \
 -train_src data/seq2seq-train-pipeline-src.txt \
 -train_tgt data/seq2seq-train-pipeline-tgt.txt \
 -valid_src data/seq2seq-dev-pipeline-src.txt \
 -valid_tgt data/seq2seq-dev-pipeline-tgt.txt \
 -save_data data/seq2seq-pipeline \
 -lower -dynamic_dict -src_seq_length 10000

 #embedding
 PYTHONPATH=seq2seq \
 python seq2seq/tools/embeddings_to_torch.py \
  -emb_file_enc ~/data/glove.840B.300d.txt \
  -emb_file_dec ~/data/glove.840B.300d.txt \
  -dict_file data/seq2seq-pipeline.vocab.pt \
  -output_file data/seq2seq-pipeline.embed \
  -enc_dec enc

#embedding
PYTHONPATH=seq2seq \
python seq2seq/tools/embeddings_to_torch.py \
 -emb_file_enc ~/data/glove.840B.300d.txt \
 -emb_file_dec ~/data/glove.840B.300d.txt \
 -dict_file data/seq2seq-pipeline.vocab.pt \
 -output_file data/seq2seq-pipeline.embed \
 -enc_dec dec



training

#DrQA
python rc/main.py \
 --trainset data/coqa.train.pipeline.json \
 --devset data/coqa.dev.pipeline.json \
 --n_history 0 --dir pipeline_models \
 --embed_file ~/data/glove.840B.300d.txt \
 --predict_raw_text n \
 --cuda_id 2

#PGNet
python seq2seq/train.py \
-data data/seq2seq-pipeline \
-save_model pipeline_models/seq2seq_copy \
-copy_attn -reuse_copy_attn \
-word_vec_size 300 \
-pre_word_vecs_enc data/seq2seq-pipeline.embed.enc.pt \
-pre_word_vecs_dec data/seq2seq-pipeline.embed.dec.pt \
-epochs 25 -gpuid 1 -seed 123 -log_file log_pgnet.txt

trans
python rc/main.py \
--testset data/coqa.dev.pipeline.json \
--n_history 0 \
--pretrained pipeline_models --cuda_id 3


python scripts/gen_pipeline_for_seq2seq.py \
--data_file data/coqa.dev.pipeline.json \
--output_file pipeline_models/pipeline-seq2seq-src.txt \
--pred_file pipeline_models/predictions.json


python seq2seq/translate.py \
-model pipeline_models/seq2seq_copy_acc_84.73_ppl_2.27_e40.pt \
-src pipeline_models/pipeline-seq2seq-src.txt \
-output pipeline_models/pred.txt \
-replace_unk -verbose -dynamic_dict -gpu 3


python scripts/gen_seq2seq_output.py \
--data_file data/coqa-dev-interro.json \
--pred_file pipeline_models/pred.txt \
--output_file pipeline_models/pipeline.prediction.json

python evaluate-v1.0.py \
--data-file data/coqa-dev-notmodify.json \
--pred-file pipeline_models/pipeline.prediction-notmodify.json


#めも
DRQAでtestの方がスコアが高いのはadditional answerも評価に用いているから
Yes/Noはseq2seqの方で出力
seq2seqのtranslationにdynamicdictが入ってなかったので挿入
  効果はない。おそらく自動で行われている
